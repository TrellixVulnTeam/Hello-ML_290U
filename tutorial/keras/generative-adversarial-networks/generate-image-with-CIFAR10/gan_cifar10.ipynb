{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 32768)             1081344   \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_1 (LeakyReLU)    (None, 32768)             0         \n",
      "_________________________________________________________________\n",
      "reshape_1 (Reshape)          (None, 16, 16, 128)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_1 (Conv2D)            (None, 16, 16, 256)       819456    \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_2 (LeakyReLU)    (None, 16, 16, 256)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_transpose_1 (Conv2DTr (None, 32, 32, 256)       1048832   \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_3 (LeakyReLU)    (None, 32, 32, 256)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 32, 32, 256)       1638656   \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_4 (LeakyReLU)    (None, 32, 32, 256)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_3 (Conv2D)            (None, 32, 32, 256)       1638656   \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_5 (LeakyReLU)    (None, 32, 32, 256)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_4 (Conv2D)            (None, 32, 32, 3)         37635     \n",
      "=================================================================\n",
      "Total params: 6,264,579\n",
      "Trainable params: 6,264,579\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/guhwanbae/anaconda3/envs/gu-keras/lib/python3.6/site-packages/ipykernel_launcher.py:29: UserWarning: Update your `Model` call to the Keras 2 API: `Model(inputs=Tensor(\"in..., outputs=Tensor(\"co...)`\n"
     ]
    }
   ],
   "source": [
    "# Step 1: Generator network\n",
    "\n",
    "import keras\n",
    "from keras import models, layers\n",
    "import numpy as np\n",
    "\n",
    "latent_dim = 32\n",
    "height, width, n_channels = (32, 32, 3)\n",
    "\n",
    "generator_input = keras.Input(shape=(latent_dim,))\n",
    "x = layers.Dense(units=128*16*16)(generator_input)\n",
    "# Using LeakyReLu, activate function, to prevent sparse gradient problem.\n",
    "x = layers.LeakyReLU()(x)\n",
    "# Convert shape of feature map as (16, 16, 128)\n",
    "x = layers.Reshape((16, 16, 128))(x)\n",
    "x = layers.Conv2D(filters=256, kernel_size=5, padding='same')(x)\n",
    "x = layers.LeakyReLU()(x)\n",
    "# Upsampling\n",
    "x = layers.Conv2DTranspose(filters=256, kernel_size=4, strides=2, padding='same')(x)\n",
    "x = layers.LeakyReLU()(x)\n",
    "x = layers.Conv2D(filters=256, kernel_size=5, padding='same')(x)\n",
    "x = layers.LeakyReLU()(x)\n",
    "x = layers.Conv2D(filters=256, kernel_size=5, padding='same')(x)\n",
    "x = layers.LeakyReLU()(x)\n",
    "# Final output has shape as (32, 32, 3). It is same as image array's shape.\n",
    "# If GAN is trained well, the generator output generated image like real image.\n",
    "x = layers.Conv2D(filters=n_channels, kernel_size=7, activation='tanh', padding='same')(x)\n",
    "\n",
    "generator = keras.models.Model(input=generator_input, output=x)\n",
    "generator.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_2 (InputLayer)         (None, 32, 32, 3)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_5 (Conv2D)            (None, 30, 30, 128)       3584      \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_6 (LeakyReLU)    (None, 30, 30, 128)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_6 (Conv2D)            (None, 14, 14, 128)       262272    \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_7 (LeakyReLU)    (None, 14, 14, 128)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_7 (Conv2D)            (None, 6, 6, 128)         262272    \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_8 (LeakyReLU)    (None, 6, 6, 128)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_8 (Conv2D)            (None, 2, 2, 128)         262272    \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_9 (LeakyReLU)    (None, 2, 2, 128)         0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 1)                 513       \n",
      "=================================================================\n",
      "Total params: 790,913\n",
      "Trainable params: 790,913\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/guhwanbae/anaconda3/envs/gu-keras/lib/python3.6/site-packages/ipykernel_launcher.py:20: UserWarning: Update your `Model` call to the Keras 2 API: `Model(inputs=Tensor(\"in..., outputs=Tensor(\"de...)`\n"
     ]
    }
   ],
   "source": [
    "# Step 2: Discriminator network\n",
    "\n",
    "# Input of the discriminator has a shape as (height, width, n_channels), image.\n",
    "discriminator_input = keras.Input(shape=(height, width, n_channels))\n",
    "x = layers.Conv2D(filters=128, kernel_size=3)(discriminator_input)\n",
    "x = layers.LeakyReLU()(x)\n",
    "# Downsampling\n",
    "x = layers.Conv2D(filters=128, kernel_size=4, strides=2)(x)\n",
    "x = layers.LeakyReLU()(x)\n",
    "x = layers.Conv2D(filters=128, kernel_size=4, strides=2)(x)\n",
    "x = layers.LeakyReLU()(x)\n",
    "x = layers.Conv2D(filters=128, kernel_size=4, strides=2)(x)\n",
    "x = layers.LeakyReLU()(x)\n",
    "x = layers.Flatten()(x)\n",
    "# Dropout layer to prevent local minimize.\n",
    "x = layers.Dropout(rate=0.45)(x)\n",
    "# Binary classification\n",
    "x = layers.Dense(units=1, activation='sigmoid')(x)\n",
    "\n",
    "discriminator = keras.models.Model(input=discriminator_input, output=x)\n",
    "discriminator.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "discriminator_optimizer = keras.optimizers.RMSprop(lr=0.0007,\n",
    "                                                   clipvalue=1.0,\n",
    "                                                   decay=1e-8)\n",
    "discriminator.compile(optimizer=discriminator_optimizer,\n",
    "                      loss='binary_crossentropy')\n",
    "discriminator.trainable = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_3 (InputLayer)         (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "model_1 (Model)              (None, 32, 32, 3)         6264579   \n",
      "_________________________________________________________________\n",
      "model_2 (Model)              (None, 1)                 790913    \n",
      "=================================================================\n",
      "Total params: 7,055,492\n",
      "Trainable params: 6,264,579\n",
      "Non-trainable params: 790,913\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/guhwanbae/anaconda3/envs/gu-keras/lib/python3.6/site-packages/ipykernel_launcher.py:6: UserWarning: Update your `Model` call to the Keras 2 API: `Model(inputs=Tensor(\"in..., outputs=Tensor(\"mo...)`\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "# Step 3: GAN, generative adversarial network\n",
    "\n",
    "gan_input = keras.Input(shape=(latent_dim,))\n",
    "gan_output = discriminator(generator(gan_input))\n",
    "\n",
    "gan = keras.models.Model(input=gan_input, output=gan_output)\n",
    "\n",
    "gan_optimizer = keras.optimizers.RMSprop(lr=0.00045,\n",
    "                                         clipvalue=1.0,\n",
    "                                         decay=1e-8)\n",
    "\n",
    "gan.compile(optimizer=gan_optimizer,\n",
    "            loss='binary_crossentropy')\n",
    "\n",
    "gan.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Choose a class truck, label 9\n",
      "The number of samples = 5000\n"
     ]
    }
   ],
   "source": [
    "# Step 4: Train a GAN\n",
    "\n",
    "from keras.datasets import cifar10\n",
    "from keras import preprocessing\n",
    "import os\n",
    "\n",
    "\n",
    "'''\n",
    "CIFAR10 Classes:\n",
    "\n",
    "0 : airplane\n",
    "1 : automobile\n",
    "2 : bird\n",
    "3 : cat\n",
    "4 : deer\n",
    "5 : dog\n",
    "6 : frog\n",
    "7 : horse\n",
    "8 : ship\n",
    "9 : truck\n",
    "'''\n",
    "class_index = {0:'airplane',\n",
    "               1:'automobile',\n",
    "               2:'bird',\n",
    "               3:'cat',\n",
    "               4:'deer',\n",
    "               5:'dog',\n",
    "               6:'frog',\n",
    "               7:'horse',\n",
    "               8:'ship',\n",
    "               9:'truck'}\n",
    "\n",
    "(train_data, train_targets), (test_data, test_targets) = cifar10.load_data()\n",
    "\n",
    "# Choose a label randomly.\n",
    "label = np.random.randint(low=0, high=10)\n",
    "\n",
    "# Choose a single class.\n",
    "train_data = train_data[train_targets.flatten() == label]\n",
    "\n",
    "class_name = class_index[label]\n",
    "n_samples = train_data.shape[0]\n",
    "print('Choose a class %s, label %d' % (class_name, label))\n",
    "print('The number of samples =', n_samples)\n",
    "\n",
    "train_data = train_data.reshape((n_samples, height, width, n_channels))\n",
    "train_data = train_data.astype('float32') / 255.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/guhwanbae/anaconda3/envs/gu-keras/lib/python3.6/site-packages/keras/engine/training.py:490: UserWarning: Discrepancy between trainable weights and collected trainable weights, did you set `model.trainable` without calling `model.compile` after ?\n",
      "  'Discrepancy between trainable weights and collected trainable'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------------\n",
      "epoch = 0, 4.232402324676514 sec\n",
      "Discriminator loss = 0.6995174\n",
      "GAN loss = 0.67411184\n",
      "--------------------------------------------------------------------------------\n",
      "epoch = 100, 26.36231541633606 sec\n",
      "Discriminator loss = 0.6672346\n",
      "GAN loss = 1.2210934\n",
      "--------------------------------------------------------------------------------\n",
      "epoch = 200, 26.460723400115967 sec\n",
      "Discriminator loss = 0.7090889\n",
      "GAN loss = 0.77718747\n",
      "--------------------------------------------------------------------------------\n",
      "epoch = 300, 26.51597237586975 sec\n",
      "Discriminator loss = 0.6875516\n",
      "GAN loss = 0.8394782\n",
      "--------------------------------------------------------------------------------\n",
      "epoch = 400, 26.482739210128784 sec\n",
      "Discriminator loss = 0.6877143\n",
      "GAN loss = 0.749619\n",
      "--------------------------------------------------------------------------------\n",
      "epoch = 500, 26.79546570777893 sec\n",
      "Discriminator loss = 0.7561489\n",
      "GAN loss = 0.90286237\n",
      "--------------------------------------------------------------------------------\n",
      "epoch = 600, 26.966764450073242 sec\n",
      "Discriminator loss = 0.6853568\n",
      "GAN loss = 0.8106049\n",
      "--------------------------------------------------------------------------------\n",
      "epoch = 700, 26.525492906570435 sec\n",
      "Discriminator loss = 0.7023805\n",
      "GAN loss = 0.757273\n",
      "--------------------------------------------------------------------------------\n",
      "epoch = 800, 26.488274812698364 sec\n",
      "Discriminator loss = 0.69013643\n",
      "GAN loss = 0.7654778\n",
      "--------------------------------------------------------------------------------\n",
      "epoch = 900, 26.479962825775146 sec\n",
      "Discriminator loss = 0.72064245\n",
      "GAN loss = 1.0557995\n",
      "--------------------------------------------------------------------------------\n",
      "epoch = 1000, 26.49262762069702 sec\n",
      "Discriminator loss = 0.5056415\n",
      "GAN loss = 7.07769\n",
      "--------------------------------------------------------------------------------\n",
      "epoch = 1100, 26.476445198059082 sec\n",
      "Discriminator loss = 0.6568081\n",
      "GAN loss = 1.0268326\n",
      "--------------------------------------------------------------------------------\n",
      "epoch = 1200, 26.534892797470093 sec\n",
      "Discriminator loss = 0.63626826\n",
      "GAN loss = 1.1044215\n",
      "--------------------------------------------------------------------------------\n",
      "epoch = 1300, 26.46579909324646 sec\n",
      "Discriminator loss = 0.7430573\n",
      "GAN loss = 1.0617253\n",
      "--------------------------------------------------------------------------------\n",
      "epoch = 1400, 26.470662355422974 sec\n",
      "Discriminator loss = 0.63215256\n",
      "GAN loss = 0.76508045\n",
      "--------------------------------------------------------------------------------\n",
      "epoch = 1500, 26.474981546401978 sec\n",
      "Discriminator loss = 0.6944529\n",
      "GAN loss = 0.795309\n",
      "--------------------------------------------------------------------------------\n",
      "epoch = 1600, 26.5094313621521 sec\n",
      "Discriminator loss = 0.663473\n",
      "GAN loss = 0.78490186\n",
      "--------------------------------------------------------------------------------\n",
      "epoch = 1700, 26.484581470489502 sec\n",
      "Discriminator loss = 0.67999464\n",
      "GAN loss = 0.79344213\n",
      "--------------------------------------------------------------------------------\n",
      "epoch = 1800, 26.472076892852783 sec\n",
      "Discriminator loss = 0.691814\n",
      "GAN loss = 0.7818166\n",
      "--------------------------------------------------------------------------------\n",
      "epoch = 1900, 26.484402418136597 sec\n",
      "Discriminator loss = 0.6818155\n",
      "GAN loss = 1.3095632\n",
      "--------------------------------------------------------------------------------\n",
      "epoch = 2000, 26.759207010269165 sec\n",
      "Discriminator loss = 0.6792243\n",
      "GAN loss = 0.95170194\n",
      "--------------------------------------------------------------------------------\n",
      "epoch = 2100, 26.761690616607666 sec\n",
      "Discriminator loss = 0.7059535\n",
      "GAN loss = 0.8119502\n",
      "--------------------------------------------------------------------------------\n",
      "epoch = 2200, 26.678918838500977 sec\n",
      "Discriminator loss = 0.6986786\n",
      "GAN loss = 0.8111372\n",
      "--------------------------------------------------------------------------------\n",
      "epoch = 2300, 26.673175573349 sec\n",
      "Discriminator loss = 0.61697614\n",
      "GAN loss = 0.8537486\n",
      "--------------------------------------------------------------------------------\n",
      "epoch = 2400, 26.686685800552368 sec\n",
      "Discriminator loss = 0.6829101\n",
      "GAN loss = 0.79202825\n",
      "--------------------------------------------------------------------------------\n",
      "epoch = 2500, 26.707053899765015 sec\n",
      "Discriminator loss = 0.7053468\n",
      "GAN loss = 0.92037886\n",
      "--------------------------------------------------------------------------------\n",
      "epoch = 2600, 26.681873083114624 sec\n",
      "Discriminator loss = 0.6573607\n",
      "GAN loss = 0.8940067\n",
      "--------------------------------------------------------------------------------\n",
      "epoch = 2700, 26.68013572692871 sec\n",
      "Discriminator loss = 0.7019798\n",
      "GAN loss = 0.7441515\n",
      "--------------------------------------------------------------------------------\n",
      "epoch = 2800, 26.68843936920166 sec\n",
      "Discriminator loss = 0.68772644\n",
      "GAN loss = 0.81303775\n",
      "--------------------------------------------------------------------------------\n",
      "epoch = 2900, 26.67249608039856 sec\n",
      "Discriminator loss = 0.69376266\n",
      "GAN loss = 0.741368\n",
      "--------------------------------------------------------------------------------\n",
      "epoch = 3000, 26.70849370956421 sec\n",
      "Discriminator loss = 0.7022177\n",
      "GAN loss = 0.90050876\n",
      "--------------------------------------------------------------------------------\n",
      "epoch = 3100, 26.669085025787354 sec\n",
      "Discriminator loss = 0.69887763\n",
      "GAN loss = 0.78783786\n",
      "--------------------------------------------------------------------------------\n",
      "epoch = 3200, 26.67691731452942 sec\n",
      "Discriminator loss = 0.68234843\n",
      "GAN loss = 0.9620981\n",
      "--------------------------------------------------------------------------------\n",
      "epoch = 3300, 26.66897940635681 sec\n",
      "Discriminator loss = 0.6828275\n",
      "GAN loss = 0.7575022\n",
      "--------------------------------------------------------------------------------\n",
      "epoch = 3400, 26.705347776412964 sec\n",
      "Discriminator loss = 0.6864453\n",
      "GAN loss = 0.8076404\n",
      "--------------------------------------------------------------------------------\n",
      "epoch = 3500, 26.67496681213379 sec\n",
      "Discriminator loss = 0.70195115\n",
      "GAN loss = 0.935887\n",
      "--------------------------------------------------------------------------------\n",
      "epoch = 3600, 26.6803777217865 sec\n",
      "Discriminator loss = 0.6881655\n",
      "GAN loss = 0.8153127\n",
      "--------------------------------------------------------------------------------\n",
      "epoch = 3700, 26.68523621559143 sec\n",
      "Discriminator loss = 0.6840607\n",
      "GAN loss = 0.7712245\n",
      "--------------------------------------------------------------------------------\n",
      "epoch = 3800, 26.666446924209595 sec\n",
      "Discriminator loss = 0.6904952\n",
      "GAN loss = 0.83696175\n",
      "--------------------------------------------------------------------------------\n",
      "epoch = 3900, 26.697649717330933 sec\n",
      "Discriminator loss = 0.7139875\n",
      "GAN loss = 0.6942874\n",
      "--------------------------------------------------------------------------------\n",
      "epoch = 4000, 26.484611749649048 sec\n",
      "Discriminator loss = 0.7331434\n",
      "GAN loss = 0.8238567\n",
      "--------------------------------------------------------------------------------\n",
      "epoch = 4100, 26.485914707183838 sec\n",
      "Discriminator loss = 0.6869766\n",
      "GAN loss = 0.8748023\n",
      "--------------------------------------------------------------------------------\n",
      "epoch = 4200, 26.480262517929077 sec\n",
      "Discriminator loss = 0.6720091\n",
      "GAN loss = 1.0694473\n",
      "--------------------------------------------------------------------------------\n",
      "epoch = 4300, 26.50838351249695 sec\n",
      "Discriminator loss = 0.70250213\n",
      "GAN loss = 0.806914\n",
      "--------------------------------------------------------------------------------\n",
      "epoch = 4400, 26.48686933517456 sec\n",
      "Discriminator loss = 0.64763445\n",
      "GAN loss = 0.87836826\n",
      "--------------------------------------------------------------------------------\n",
      "epoch = 4500, 26.481534957885742 sec\n",
      "Discriminator loss = 0.7229715\n",
      "GAN loss = 0.80597925\n",
      "--------------------------------------------------------------------------------\n",
      "epoch = 4600, 26.493351936340332 sec\n",
      "Discriminator loss = 0.67832744\n",
      "GAN loss = 0.7837044\n",
      "--------------------------------------------------------------------------------\n",
      "epoch = 4700, 26.474536895751953 sec\n",
      "Discriminator loss = 0.703653\n",
      "GAN loss = 0.8535469\n",
      "--------------------------------------------------------------------------------\n",
      "epoch = 4800, 26.52457547187805 sec\n",
      "Discriminator loss = 0.7637514\n",
      "GAN loss = 0.5883978\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------------\n",
      "epoch = 4900, 26.48762607574463 sec\n",
      "Discriminator loss = 0.6682862\n",
      "GAN loss = 0.8036044\n",
      "--------------------------------------------------------------------------------\n",
      "epoch = 5000, 26.474180698394775 sec\n",
      "Discriminator loss = 0.62473184\n",
      "GAN loss = 0.89237815\n",
      "--------------------------------------------------------------------------------\n",
      "epoch = 5100, 26.48740315437317 sec\n",
      "Discriminator loss = 0.70363945\n",
      "GAN loss = 0.8042613\n",
      "--------------------------------------------------------------------------------\n",
      "epoch = 5200, 26.8161301612854 sec\n",
      "Discriminator loss = 0.71689135\n",
      "GAN loss = 0.8072203\n",
      "--------------------------------------------------------------------------------\n",
      "epoch = 5300, 26.686927556991577 sec\n",
      "Discriminator loss = 0.70812607\n",
      "GAN loss = 0.81318474\n",
      "--------------------------------------------------------------------------------\n",
      "epoch = 5400, 26.699541568756104 sec\n",
      "Discriminator loss = 0.69103396\n",
      "GAN loss = 0.7043787\n",
      "--------------------------------------------------------------------------------\n",
      "epoch = 5500, 26.67551302909851 sec\n",
      "Discriminator loss = 0.6004058\n",
      "GAN loss = 1.3857397\n",
      "--------------------------------------------------------------------------------\n",
      "epoch = 5600, 26.554917812347412 sec\n",
      "Discriminator loss = 0.65021783\n",
      "GAN loss = 0.7010748\n",
      "--------------------------------------------------------------------------------\n",
      "epoch = 5700, 26.681365251541138 sec\n",
      "Discriminator loss = 0.6733599\n",
      "GAN loss = 1.0930903\n",
      "--------------------------------------------------------------------------------\n",
      "epoch = 5800, 26.627346515655518 sec\n",
      "Discriminator loss = 0.77980155\n",
      "GAN loss = 1.041336\n",
      "--------------------------------------------------------------------------------\n",
      "epoch = 5900, 26.62186098098755 sec\n",
      "Discriminator loss = 0.6668\n",
      "GAN loss = 1.1463492\n",
      "--------------------------------------------------------------------------------\n",
      "epoch = 6000, 26.621216535568237 sec\n",
      "Discriminator loss = 0.7865348\n",
      "GAN loss = 0.7220219\n",
      "--------------------------------------------------------------------------------\n",
      "epoch = 6100, 26.670336723327637 sec\n",
      "Discriminator loss = 0.6906404\n",
      "GAN loss = 0.8658115\n",
      "--------------------------------------------------------------------------------\n",
      "epoch = 6200, 26.629342317581177 sec\n",
      "Discriminator loss = 0.5980728\n",
      "GAN loss = 0.9070307\n",
      "--------------------------------------------------------------------------------\n",
      "epoch = 6300, 26.630960941314697 sec\n",
      "Discriminator loss = 0.6763697\n",
      "GAN loss = 0.8111502\n",
      "--------------------------------------------------------------------------------\n",
      "epoch = 6400, 26.64024043083191 sec\n",
      "Discriminator loss = 0.6975461\n",
      "GAN loss = 0.87023795\n",
      "--------------------------------------------------------------------------------\n",
      "epoch = 6500, 26.630321741104126 sec\n",
      "Discriminator loss = 0.77228814\n",
      "GAN loss = 1.1040373\n",
      "--------------------------------------------------------------------------------\n",
      "epoch = 6600, 26.680607795715332 sec\n",
      "Discriminator loss = 0.6982754\n",
      "GAN loss = 1.4086288\n",
      "--------------------------------------------------------------------------------\n",
      "epoch = 6700, 26.638453245162964 sec\n",
      "Discriminator loss = 0.50319153\n",
      "GAN loss = 1.6618919\n",
      "--------------------------------------------------------------------------------\n",
      "epoch = 6800, 26.636518239974976 sec\n",
      "Discriminator loss = 0.6330013\n",
      "GAN loss = 0.8264227\n",
      "--------------------------------------------------------------------------------\n",
      "epoch = 6900, 26.6314594745636 sec\n",
      "Discriminator loss = 0.5693985\n",
      "GAN loss = 1.5223103\n",
      "--------------------------------------------------------------------------------\n",
      "epoch = 7000, 26.67790174484253 sec\n",
      "Discriminator loss = 0.74141806\n",
      "GAN loss = 1.2317489\n",
      "--------------------------------------------------------------------------------\n",
      "epoch = 7100, 26.63770294189453 sec\n",
      "Discriminator loss = 0.4679849\n",
      "GAN loss = 1.2638836\n",
      "--------------------------------------------------------------------------------\n",
      "epoch = 7200, 26.62897038459778 sec\n",
      "Discriminator loss = 0.6927757\n",
      "GAN loss = 1.0215013\n",
      "--------------------------------------------------------------------------------\n",
      "epoch = 7300, 26.624412298202515 sec\n",
      "Discriminator loss = 0.678781\n",
      "GAN loss = 0.9025019\n",
      "--------------------------------------------------------------------------------\n",
      "epoch = 7400, 26.637086629867554 sec\n",
      "Discriminator loss = 0.6368969\n",
      "GAN loss = 1.0816879\n",
      "--------------------------------------------------------------------------------\n",
      "epoch = 7500, 26.674046754837036 sec\n",
      "Discriminator loss = 0.708635\n",
      "GAN loss = 0.65027744\n",
      "--------------------------------------------------------------------------------\n",
      "epoch = 7600, 26.61832356452942 sec\n",
      "Discriminator loss = 0.38194403\n",
      "GAN loss = 4.5858274\n",
      "--------------------------------------------------------------------------------\n",
      "epoch = 7700, 26.64728856086731 sec\n",
      "Discriminator loss = 0.38579956\n",
      "GAN loss = 1.5584613\n",
      "--------------------------------------------------------------------------------\n",
      "epoch = 7800, 26.643300771713257 sec\n",
      "Discriminator loss = 0.6471861\n",
      "GAN loss = 0.7979678\n",
      "--------------------------------------------------------------------------------\n",
      "epoch = 7900, 26.65754222869873 sec\n",
      "Discriminator loss = 0.76195896\n",
      "GAN loss = 0.9308588\n",
      "--------------------------------------------------------------------------------\n",
      "epoch = 8000, 26.64267897605896 sec\n",
      "Discriminator loss = 0.72804654\n",
      "GAN loss = 0.9485086\n",
      "--------------------------------------------------------------------------------\n",
      "epoch = 8100, 26.62903118133545 sec\n",
      "Discriminator loss = 0.5945836\n",
      "GAN loss = 2.01304\n",
      "--------------------------------------------------------------------------------\n",
      "epoch = 8200, 26.641345024108887 sec\n",
      "Discriminator loss = 0.47183496\n",
      "GAN loss = 1.2820852\n",
      "--------------------------------------------------------------------------------\n",
      "epoch = 8300, 26.628071546554565 sec\n",
      "Discriminator loss = 0.3605203\n",
      "GAN loss = 3.340894\n",
      "--------------------------------------------------------------------------------\n",
      "epoch = 8400, 26.67293095588684 sec\n",
      "Discriminator loss = 0.3938063\n",
      "GAN loss = 2.3552463\n",
      "--------------------------------------------------------------------------------\n",
      "epoch = 8500, 26.63192367553711 sec\n",
      "Discriminator loss = 0.80705166\n",
      "GAN loss = 1.2518073\n",
      "--------------------------------------------------------------------------------\n",
      "epoch = 8600, 27.343759536743164 sec\n",
      "Discriminator loss = 0.7047633\n",
      "GAN loss = 1.5345074\n",
      "--------------------------------------------------------------------------------\n",
      "epoch = 8700, 26.596469402313232 sec\n",
      "Discriminator loss = 0.68171895\n",
      "GAN loss = 1.4137372\n",
      "--------------------------------------------------------------------------------\n",
      "epoch = 8800, 26.629683256149292 sec\n",
      "Discriminator loss = 0.38362134\n",
      "GAN loss = 1.9362209\n",
      "--------------------------------------------------------------------------------\n",
      "epoch = 8900, 26.601083993911743 sec\n",
      "Discriminator loss = 0.43373424\n",
      "GAN loss = 2.7999103\n",
      "--------------------------------------------------------------------------------\n",
      "epoch = 9000, 26.603559255599976 sec\n",
      "Discriminator loss = 0.75308335\n",
      "GAN loss = 0.69770896\n",
      "--------------------------------------------------------------------------------\n",
      "epoch = 9100, 26.595468997955322 sec\n",
      "Discriminator loss = 0.46450335\n",
      "GAN loss = 3.3131592\n",
      "--------------------------------------------------------------------------------\n",
      "epoch = 9200, 26.598110914230347 sec\n",
      "Discriminator loss = 0.37368447\n",
      "GAN loss = 1.4295362\n",
      "--------------------------------------------------------------------------------\n",
      "epoch = 9300, 26.628015279769897 sec\n",
      "Discriminator loss = 0.5184089\n",
      "GAN loss = 1.0956632\n",
      "--------------------------------------------------------------------------------\n",
      "epoch = 9400, 26.603348970413208 sec\n",
      "Discriminator loss = 0.5192462\n",
      "GAN loss = 1.4839672\n",
      "--------------------------------------------------------------------------------\n",
      "epoch = 9500, 26.604256868362427 sec\n",
      "Discriminator loss = 0.6451422\n",
      "GAN loss = 0.8629077\n",
      "--------------------------------------------------------------------------------\n",
      "epoch = 9600, 26.606211185455322 sec\n",
      "Discriminator loss = 0.5292523\n",
      "GAN loss = 1.7011926\n",
      "--------------------------------------------------------------------------------\n",
      "epoch = 9700, 26.63208508491516 sec\n",
      "Discriminator loss = 0.11777276\n",
      "GAN loss = 10.697461\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------------\n",
      "epoch = 9800, 26.611857414245605 sec\n",
      "Discriminator loss = 0.5236918\n",
      "GAN loss = 2.353588\n",
      "--------------------------------------------------------------------------------\n",
      "epoch = 9900, 26.597471952438354 sec\n",
      "Discriminator loss = 0.64740384\n",
      "GAN loss = 1.0290115\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "save_dir = './gan-generated-' + class_name\n",
    "if not os.path.exists(save_dir):\n",
    "    os.mkdir(save_dir)\n",
    "\n",
    "iterations = 10000\n",
    "batch_size = 20\n",
    "\n",
    "begin_epoch = time.time()\n",
    "start = 0\n",
    "for step in range(iterations):\n",
    "    # Choose a sample in latent space.\n",
    "    random_latent_vectors = np.random.normal(size=(batch_size, latent_dim))\n",
    "\n",
    "    # Decoding fake images.\n",
    "    fake_images = generator.predict(random_latent_vectors)\n",
    "    \n",
    "    stop = start + batch_size\n",
    "    real_images = train_data[start:stop]\n",
    "    \n",
    "    combined_images = np.concatenate([fake_images, real_images])\n",
    "    \n",
    "    labels = np.concatenate([np.ones((batch_size, 1)), np.zeros((batch_size, 1))])\n",
    "    labels += 0.05 * np.random.random(labels.shape)\n",
    "    \n",
    "    discriminator_loss = discriminator.train_on_batch(combined_images, labels)\n",
    "    \n",
    "    random_latent_vectors = np.random.normal(size=(batch_size, latent_dim))\n",
    "    \n",
    "    misleading_targets = np.zeros((batch_size, 1))\n",
    "    gan_loss = gan.train_on_batch(random_latent_vectors, misleading_targets)\n",
    "    \n",
    "    start += batch_size\n",
    "    \n",
    "    if start > (n_samples - batch_size):\n",
    "        start = 0\n",
    "    if (step % 100) == 0:\n",
    "        end_epoch = time.time()\n",
    "        \n",
    "        gan.save_weights('gan.h5')\n",
    "        print('-'*80)\n",
    "        print('epoch = %d, %s sec' % (step, (end_epoch - begin_epoch)))\n",
    "        print('Discriminator loss =', discriminator_loss)\n",
    "        print('GAN loss =', gan_loss)\n",
    "        \n",
    "        # Save a fake iamge.\n",
    "        img = preprocessing.image.array_to_img(fake_images[0] * 255.0, scale=False)\n",
    "        img.save(os.path.join(save_dir, 'fake_' + class_name + '_' + str(step) + '.png'))\n",
    "        # Save a real image.\n",
    "        img = preprocessing.image.array_to_img(real_images[0] * 255.0, scale=False)\n",
    "        img.save(os.path.join(save_dir, 'real_' + class_name + '_' + str(step) + '.png'))\n",
    "        \n",
    "        begin_epoch = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python gu-keras",
   "language": "python",
   "name": "gu-keras"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
